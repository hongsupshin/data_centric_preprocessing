# Description

Hardware verification data pose many challenges for ML. In addition to their high heterogeneity and dimensionality, the number and dtype of features change over time. Besides, raw data often have inaccurate dtypes (e.g., object for float), which requires type inference. Unfortunately, schemas are absent and feature meanings are highly obscure.

These issues can cause serious ML problems. Data preprocessing pipeline gets complicated, opaque, and inefficient, which becomes a bottleneck. The ML systems become more vulnerable to data leakage, bringing overoptimism in performance. Trained models frequently fail during serving because the feature set changes. Finally, without a good understanding of the data, non-standardized and hacky preprocessing methods can easily thrive. To address these, I adopt a data-centric approach to build a streamlined and automated ML pipeline that utilizes standard python packages (pandas, numpy, scikit-learn, and prefect). It is adaptive to data drifts, and increases modularity, transparency, and efficiency of data preprocessing. 

When the raw data arrive from our ETL pipeline, all features have object dtypes although this is inaccurate. For instance, a numeric array of [0, 0, 1] is represented as object; array([0, 0, 1], dtype=object). Sklearn preprocessors can handle this, but true dtypes must be inferred because the data are heterogeneous and thus separate preprocessing methods should be applied to different feature subsets.

Among various dtype inference methods, I choose pandas.api.types.infer_dtype because of its type granularity and capability to ignore null values. However, because it returns a string label for a dtype (“string” for str), the label should be mapped to numpy dtypes for casting (e.g., {“string”: str}). Also, the label should be mapped to domain-specific custom types to build the data preprocessor. Feature names and their dtypes (pandas, numpy, and custom) are saved as a schema, and the numpy dtypes are used to correct the raw data dtypes. Compared to our current data preprocessing method, this approach decreased the training runtime by 10x on average.

Data preprocessing is done by a sklearn Pipeline with pre-defined steps (“preprocessor”). The pipeline has a “column transformer” step that handles data heterogeneity. It consists of multiple column transformers (sklearn ColumnTransformer). Each transformer handles a feature subset of a specific custom dtype. A single transformer requires a feature subset (i.e., feature names) and its preprocessing method. The method comes from a user-defined look-up table (“transformer map”) and the feature names are fetched from the schema. Since the schema is inferred from the training data, the preprocessor structure can adapt to any changes in them.

During serving, the feature set is likely to have changed, which should be addressed to allow model prediction. A schema is inferred from the serving data, and by comparing it with the training schema, feature-set and type mismatches are resolved. Unseen features are dropped, and missing features are added back as invariant dummies whose dtypes are from the training schema. The intention behind this simple resolution is not to ignore data drifts but to avoid frequent pipeline failures because mismatches occur often. Whenever the mismatches are detected, manual data inspection is triggered.

In addition to high non-stationarity, verification data are highly domain-specific and abstract. Thus, features can have multiple interpretations. Since the data preprocessing and model pipelines can be merged, preprocessing methods can be tuned as if model hyperparameters (i.e., “data tuning”). I tuned 16 preprocessing methods for 52 real-world benchmark datasets without any model tuning. Performance comparison between the best and the worst methods resulted in mean difference of 0.11 AUROC across benchmark datasets. This demonstrates that choosing the right data preprocessing methods has a sizable impact on model performance.

This data-centric ML pipeline is built with commonly used python ML packages, and it resolves complex data problems from hardware verification data such as mislabeled types and data drifts. It increases transparency and flexibility of the pipeline, which empowers ML practitioners to adapt to data drifts in the future and to find better representation of their data. In fact, the benchmark test shows that optimizing data preprocessing methods can improve model performance. This highlights the importance of data handling when building robust and high-performing ML systems.
